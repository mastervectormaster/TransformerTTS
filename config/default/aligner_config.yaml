# ARCHITECTURE
decoder_model_dimension: 256
encoder_model_dimension: 256
decoder_num_heads: [4, 4, 4, 1]  # the length of this defines the number of layers
encoder_num_heads: [4, 4, 4, 4]  # the length of this defines the number of layers
encoder_feed_forward_dimension: 512
decoder_feed_forward_dimension: 512
decoder_prenet_dimension: 256
encoder_prenet_dimension: 256
encoder_max_position_encoding: 10000
decoder_max_position_encoding: 10000

# LOSSES
stop_loss_scaling: 8

# TRAINING
dropout_rate: 0.1
decoder_prenet_dropout: 0.1
learning_rate_schedule:
  - [0, 1.0e-4]
head_drop_schedule:  # head-level dropout: how many heads to set to zero at training time
  - [0, 0]
reduction_factor_schedule:
  - [0, 10]
  - [80_000, 1]
max_steps: 160_000
bucket_boundaries: [200, 300, 400, 500, 600, 700, 800, 900, 1000, 1200] # mel bucketing
bucket_batch_sizes: [64, 42, 32, 25, 21, 18, 16, 14, 12, 11, 1]
val_bucket_batch_size: [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 1]
force_encoder_diagonal_steps: 1_000
force_decoder_diagonal_steps: 7_000
use_pe_target_mel: False  # add positional encodings to target mels
extract_attention_weighted: False # weighted average between last layer decoder attention heads when extracting durations
debug: False

# LOGGING
validation_frequency: 5_000
weights_save_frequency: 5_000
train_images_plotting_frequency: 1_000
keep_n_weights: 2
keep_checkpoint_every_n_hours: 12
n_steps_avg_losses: [100, 500, 1_000, 5_000]  # command line display of average loss values for the last n steps
prediction_start_step: 30_000 # step after which to predict durations at validation time